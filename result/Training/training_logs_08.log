Read configuration file: configurations/PEMS08_astgcn.conf
CUDA: True cuda:0
folder_dir: astgcn_r_h1d0w0_channel1_1.000000e-03
params_path: experiments/PEMS08/astgcn_r_h1d0w0_channel1_1.000000e-03
load file: ./data/PEMS08/PEMS08_r1_d0_w0_astcgn
train: torch.Size([10699, 170, 1, 12]) torch.Size([10699, 170, 12])
val: torch.Size([3567, 170, 1, 12]) torch.Size([3567, 170, 12])
test: torch.Size([3567, 170, 1, 12]) torch.Size([3567, 170, 12])
create params directory experiments/PEMS08/astgcn_r_h1d0w0_channel1_1.000000e-03
param list:
CUDA     cuda:0
in_channels      1
nb_block         2
nb_chev_filter   64
nb_time_filter   64
time_strides     1
batch_size       32
graph_signal_matrix_filename     ./data/PEMS08/PEMS08.npz
start_epoch      0
epochs   80
ASTGCN_submodule(
  (BlockList): ModuleList(
    (0): ASTGCN_block(
      (TAt): Temporal_Attention_layer()
      (SAt): Spatial_Attention_layer()
      (cheb_conv_SAt): cheb_conv_withSAt(
        (Theta): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 1x64 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 1x64 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 1x64 (GPU 0)]
        )
      )
      (time_conv): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
      (residual_conv): Conv2d(1, 64, kernel_size=(1, 1), stride=(1, 1))
      (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
    (1): ASTGCN_block(
      (TAt): Temporal_Attention_layer()
      (SAt): Spatial_Attention_layer()
      (cheb_conv_SAt): cheb_conv_withSAt(
        (Theta): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 64x64 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 64x64 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 64x64 (GPU 0)]
        )
      )
      (time_conv): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
      (residual_conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
      (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (final_conv): Conv2d(12, 12, kernel_size=(1, 64), stride=(1, 1))
)
Net's state_dict:
BlockList.0.TAt.U1       torch.Size([170])
BlockList.0.TAt.U2       torch.Size([1, 170])
BlockList.0.TAt.U3       torch.Size([1])
BlockList.0.TAt.be       torch.Size([1, 12, 12])
BlockList.0.TAt.Ve       torch.Size([12, 12])
BlockList.0.SAt.W1       torch.Size([12])
BlockList.0.SAt.W2       torch.Size([1, 12])
BlockList.0.SAt.W3       torch.Size([1])
BlockList.0.SAt.bs       torch.Size([1, 170, 170])
BlockList.0.SAt.Vs       torch.Size([170, 170])
BlockList.0.cheb_conv_SAt.Theta.0        torch.Size([1, 64])
BlockList.0.cheb_conv_SAt.Theta.1        torch.Size([1, 64])
BlockList.0.cheb_conv_SAt.Theta.2        torch.Size([1, 64])
BlockList.0.time_conv.weight     torch.Size([64, 64, 1, 3])
BlockList.0.time_conv.bias       torch.Size([64])
BlockList.0.residual_conv.weight         torch.Size([64, 1, 1, 1])
BlockList.0.residual_conv.bias   torch.Size([64])
BlockList.0.ln.weight    torch.Size([64])
BlockList.0.ln.bias      torch.Size([64])
BlockList.1.TAt.U1       torch.Size([170])
BlockList.1.TAt.U2       torch.Size([64, 170])
BlockList.1.TAt.U3       torch.Size([64])
BlockList.1.TAt.be       torch.Size([1, 12, 12])
BlockList.1.TAt.Ve       torch.Size([12, 12])
BlockList.1.SAt.W1       torch.Size([12])
BlockList.1.SAt.W2       torch.Size([64, 12])
BlockList.1.SAt.W3       torch.Size([64])
BlockList.1.SAt.bs       torch.Size([1, 170, 170])
BlockList.1.SAt.Vs       torch.Size([170, 170])
BlockList.1.cheb_conv_SAt.Theta.0        torch.Size([64, 64])
BlockList.1.cheb_conv_SAt.Theta.1        torch.Size([64, 64])
BlockList.1.cheb_conv_SAt.Theta.2        torch.Size([64, 64])
BlockList.1.time_conv.weight     torch.Size([64, 64, 1, 3])
BlockList.1.time_conv.bias       torch.Size([64])
BlockList.1.residual_conv.weight         torch.Size([64, 64, 1, 1])
BlockList.1.residual_conv.bias   torch.Size([64])
BlockList.1.ln.weight    torch.Size([64])
BlockList.1.ln.bias      torch.Size([64])
final_conv.weight        torch.Size([12, 12, 1, 64])
final_conv.bias          torch.Size([12])
Net's total params: 179456
Optimizer's state_dict:
state    {}
param_groups     [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]}]
validation batch 1 / 112, loss: 105.33
validation batch 101 / 112, loss: 295.66
save parameters to file: experiments/PEMS08/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_0.params
validation batch 1 / 112, loss: 26.71
validation batch 101 / 112, loss: 54.37
save parameters to file: experiments/PEMS08/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_1.params
validation batch 1 / 112, loss: 22.52
validation batch 101 / 112, loss: 34.60
save parameters to file: experiments/PEMS08/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_2.params
global step: 1000, training loss: 23.40, time: 52.03s
validation batch 1 / 112, loss: 22.03
validation batch 101 / 112, loss: 29.89
save parameters to file: experiments/PEMS08/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_3.params
validation batch 1 / 112, loss: 19.94
validation batch 101 / 112, loss: 29.21
save parameters to file: experiments/PEMS08/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_4.params
validation batch 1 / 112, loss: 20.31
validation batch 101 / 112, loss: 29.73
save parameters to file: experiments/PEMS08/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_5.params
global step: 2000, training loss: 23.50, time: 102.50s
validation batch 1 / 112, loss: 20.50
validation batch 101 / 112, loss: 29.46
save parameters to file: experiments/PEMS08/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_6.params
validation batch 1 / 112, loss: 19.92
validation batch 101 / 112, loss: 27.97
save parameters to file: experiments/PEMS08/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_7.params
validation batch 1 / 112, loss: 17.22
validation batch 101 / 112, loss: 26.37
save parameters to file: experiments/PEMS08/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_8.params
global step: 3000, training loss: 25.70, time: 152.14s
validation batch 1 / 112, loss: 16.49
validation batch 101 / 112, loss: 26.59
save parameters to file: experiments/PEMS08/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_9.params
validation batch 1 / 112, loss: 16.43
validation batch 101 / 112, loss: 26.50
save parameters to file: experiments/PEMS08/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_10.params
validation batch 1 / 112, loss: 16.87
validation batch 101 / 112, loss: 26.51
save parameters to file: experiments/PEMS08/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_11.params
global step: 4000, training loss: 18.60, time: 202.16s
validation batch 1 / 112, loss: 16.34
validation batch 101 / 112, loss: 26.43
save parameters to file: experiments/PEMS08/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_12.params
validation batch 1 / 112, loss: 15.79
validation batch 101 / 112, loss: 25.35
save parameters to file: experiments/PEMS08/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_13.params
validation batch 1 / 112, loss: 14.95
validation batch 101 / 112, loss: 24.89
save parameters to file: experiments/PEMS08/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_14.params
global step: 5000, training loss: 19.43, time: 252.15s
validation batch 1 / 112, loss: 15.29
validation batch 101 / 112, loss: 24.19
save parameters to file: experiments/PEMS08/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_15.params
validation batch 1 / 112, loss: 14.52
validation batch 101 / 112, loss: 24.03
save parameters to file: experiments/PEMS08/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_16.params
validation batch 1 / 112, loss: 14.70
validation batch 101 / 112, loss: 23.63
save parameters to file: experiments/PEMS08/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_17.params
global step: 6000, training loss: 19.29, time: 301.84s
validation batch 1 / 112, loss: 14.54
validation batch 101 / 112, loss: 23.68
save parameters to file: experiments/PEMS08/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_18.params
validation batch 1 / 112, loss: 14.13
validation batch 101 / 112, loss: 24.18
validation batch 1 / 112, loss: 14.25
validation batch 101 / 112, loss: 23.71
save parameters to file: experiments/PEMS08/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_20.params
global step: 7000, training loss: 19.70, time: 351.78s
validation batch 1 / 112, loss: 14.06
validation batch 101 / 112, loss: 24.02
validation batch 1 / 112, loss: 14.09
validation batch 101 / 112, loss: 23.82
save parameters to file: experiments/PEMS08/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_22.params
validation batch 1 / 112, loss: 13.63
validation batch 101 / 112, loss: 24.26
save parameters to file: experiments/PEMS08/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_23.params
global step: 8000, training loss: 18.21, time: 401.76s
validation batch 1 / 112, loss: 13.57
validation batch 101 / 112, loss: 23.22
save parameters to file: experiments/PEMS08/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_24.params
validation batch 1 / 112, loss: 13.72
validation batch 101 / 112, loss: 23.24
validation batch 1 / 112, loss: 13.47
validation batch 101 / 112, loss: 23.75
save parameters to file: experiments/PEMS08/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_26.params
global step: 9000, training loss: 17.88, time: 451.81s
validation batch 1 / 112, loss: 13.68
validation batch 101 / 112, loss: 23.58
save parameters to file: experiments/PEMS08/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_27.params
validation batch 1 / 112, loss: 13.46
validation batch 101 / 112, loss: 24.40
validation batch 1 / 112, loss: 13.68
validation batch 101 / 112, loss: 24.66
global step: 10000, training loss: 17.69, time: 501.60s
validation batch 1 / 112, loss: 13.62
validation batch 101 / 112, loss: 23.72
save parameters to file: experiments/PEMS08/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_30.params
validation batch 1 / 112, loss: 13.56
validation batch 101 / 112, loss: 24.90
validation batch 1 / 112, loss: 14.06
validation batch 101 / 112, loss: 24.19
global step: 11000, training loss: 18.16, time: 551.26s
validation batch 1 / 112, loss: 13.58
validation batch 101 / 112, loss: 23.72
save parameters to file: experiments/PEMS08/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_33.params
validation batch 1 / 112, loss: 13.03
validation batch 101 / 112, loss: 24.66
validation batch 1 / 112, loss: 13.59
validation batch 101 / 112, loss: 24.76
global step: 12000, training loss: 16.72, time: 601.10s
validation batch 1 / 112, loss: 13.75
validation batch 101 / 112, loss: 25.15
validation batch 1 / 112, loss: 13.78
validation batch 101 / 112, loss: 24.16
validation batch 1 / 112, loss: 13.00
validation batch 101 / 112, loss: 24.29
global step: 13000, training loss: 18.27, time: 650.93s
validation batch 1 / 112, loss: 13.23
validation batch 101 / 112, loss: 24.28
validation batch 1 / 112, loss: 12.64
validation batch 101 / 112, loss: 24.80
validation batch 1 / 112, loss: 12.75
validation batch 101 / 112, loss: 24.96
global step: 14000, training loss: 17.41, time: 700.97s
validation batch 1 / 112, loss: 13.02
validation batch 101 / 112, loss: 24.38
validation batch 1 / 112, loss: 12.62
validation batch 101 / 112, loss: 24.59
validation batch 1 / 112, loss: 13.21
validation batch 101 / 112, loss: 24.54
save parameters to file: experiments/PEMS08/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_44.params
global step: 15000, training loss: 17.09, time: 750.79s
validation batch 1 / 112, loss: 12.65
validation batch 101 / 112, loss: 25.12
validation batch 1 / 112, loss: 13.57
validation batch 101 / 112, loss: 24.25
save parameters to file: experiments/PEMS08/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_46.params
validation batch 1 / 112, loss: 13.97
validation batch 101 / 112, loss: 25.42
global step: 16000, training loss: 17.17, time: 800.58s
validation batch 1 / 112, loss: 13.51
validation batch 101 / 112, loss: 24.66
validation batch 1 / 112, loss: 14.48
validation batch 101 / 112, loss: 26.58
validation batch 1 / 112, loss: 12.42
validation batch 101 / 112, loss: 25.45
global step: 17000, training loss: 17.63, time: 850.31s
validation batch 1 / 112, loss: 13.25
validation batch 101 / 112, loss: 25.28
validation batch 1 / 112, loss: 12.61
validation batch 101 / 112, loss: 24.74
validation batch 1 / 112, loss: 13.03
validation batch 101 / 112, loss: 26.13
global step: 18000, training loss: 16.28, time: 900.07s
validation batch 1 / 112, loss: 12.58
validation batch 101 / 112, loss: 24.88
validation batch 1 / 112, loss: 12.48
validation batch 101 / 112, loss: 25.90
validation batch 1 / 112, loss: 13.21
validation batch 101 / 112, loss: 25.49
global step: 19000, training loss: 17.40, time: 949.81s
validation batch 1 / 112, loss: 13.30
validation batch 101 / 112, loss: 25.27
validation batch 1 / 112, loss: 12.96
validation batch 101 / 112, loss: 25.52
validation batch 1 / 112, loss: 12.95
validation batch 101 / 112, loss: 25.74
global step: 20000, training loss: 17.22, time: 999.37s
validation batch 1 / 112, loss: 13.79
validation batch 101 / 112, loss: 25.39
validation batch 1 / 112, loss: 12.31
validation batch 101 / 112, loss: 26.25
validation batch 1 / 112, loss: 13.92
validation batch 101 / 112, loss: 25.40
global step: 21000, training loss: 18.37, time: 1048.98s
validation batch 1 / 112, loss: 12.80
validation batch 101 / 112, loss: 25.99
validation batch 1 / 112, loss: 13.21
validation batch 101 / 112, loss: 25.41
save parameters to file: experiments/PEMS08/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_64.params
validation batch 1 / 112, loss: 13.31
validation batch 101 / 112, loss: 25.71
global step: 22000, training loss: 16.93, time: 1098.77s
validation batch 1 / 112, loss: 13.22
validation batch 101 / 112, loss: 26.25
validation batch 1 / 112, loss: 13.11
validation batch 101 / 112, loss: 25.72
validation batch 1 / 112, loss: 13.50
validation batch 101 / 112, loss: 25.76
global step: 23000, training loss: 16.53, time: 1148.67s
validation batch 1 / 112, loss: 12.16
validation batch 101 / 112, loss: 25.97
validation batch 1 / 112, loss: 12.78
validation batch 101 / 112, loss: 26.69
validation batch 1 / 112, loss: 12.10
validation batch 101 / 112, loss: 25.90
global step: 24000, training loss: 16.24, time: 1198.39s
validation batch 1 / 112, loss: 12.49
validation batch 101 / 112, loss: 26.08
validation batch 1 / 112, loss: 13.22
validation batch 101 / 112, loss: 25.93
validation batch 1 / 112, loss: 12.74
validation batch 101 / 112, loss: 26.07
global step: 25000, training loss: 17.16, time: 1248.28s
validation batch 1 / 112, loss: 12.77
validation batch 101 / 112, loss: 25.98
validation batch 1 / 112, loss: 12.64
validation batch 101 / 112, loss: 26.05
validation batch 1 / 112, loss: 12.34
validation batch 101 / 112, loss: 26.51
global step: 26000, training loss: 16.67, time: 1298.48s
validation batch 1 / 112, loss: 12.34
validation batch 101 / 112, loss: 26.55
validation batch 1 / 112, loss: 14.43
validation batch 101 / 112, loss: 25.93
best epoch: 64
load weight from: experiments/PEMS08/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_64.params
predicting data set batch 1 / 112
predicting data set batch 101 / 112
input: (3567, 170, 1, 12)
prediction: (3567, 170, 12)
data_target_tensor: (3567, 170, 12)
current epoch: 64, predict 0 points
MAE: 14.25
RMSE: 21.79
MAPE: 0.10
current epoch: 64, predict 1 points
MAE: 15.57
RMSE: 23.92
MAPE: 0.10
current epoch: 64, predict 2 points
MAE: 16.60
RMSE: 25.54
MAPE: 0.11
current epoch: 64, predict 3 points
MAE: 17.41
RMSE: 26.80
MAPE: 0.11
current epoch: 64, predict 4 points
MAE: 18.10
RMSE: 27.83
MAPE: 0.12
current epoch: 64, predict 5 points
MAE: 18.78
RMSE: 28.84
MAPE: 0.12
current epoch: 64, predict 6 points
MAE: 19.43
RMSE: 29.79
MAPE: 0.12
current epoch: 64, predict 7 points
MAE: 20.11
RMSE: 30.75
MAPE: 0.13
current epoch: 64, predict 8 points
MAE: 20.69
RMSE: 31.56
MAPE: 0.13
current epoch: 64, predict 9 points
MAE: 21.38
RMSE: 32.49
MAPE: 0.13
current epoch: 64, predict 10 points
MAE: 22.13
RMSE: 33.54
MAPE: 0.14
current epoch: 64, predict 11 points
MAE: 23.13
RMSE: 34.92
MAPE: 0.15
all MAE: 18.97
all RMSE: 29.23
all MAPE: 0.12
[14.2488365, 21.787858658255274, 0.09539078, 15.5657425, 23.91523906775696, 0.10028001, 16.60138, 25.540509528063943, 0.10556944, 17.40742, 26.797119851286606, 0.110155925, 18.099865, 27.831782071252977, 0.11591993, 18.782343, 28.838323907280916, 0.11897074, 19.43193, 29.788426306149088, 0.12231658, 20.105558, 30.751400303164548, 0.12572081, 20.693583, 31.562982476200958, 0.12977375, 21.375916, 32.49295108353791, 0.13407874, 22.133541, 33.539290887004825, 0.13864873, 23.13496, 34.923112394184955, 0.14516315, 18.96508, 29.230164673765845, 0.1201657]